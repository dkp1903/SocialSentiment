{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HateContentDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "otCbxMpJu0FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## IE-406 ML Project"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAY882jkR-ap",
        "colab_type": "code",
        "outputId": "4efae678-9c79-45e0-9bb7-c9d337763954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## Using Google Colab for GPU usage\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8crY2XLmvQXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Importing the necessary libraries\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "!pip install Unidecode\n",
        "!pip install Word2number\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import unidecode\n",
        "from word2number import w2n\n",
        "from pycontractions import Contractions\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn07RmHbBCIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/as-9')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIAviZW8rnn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train data prep\n",
        "train_set = pd.read_csv('train.tsv', delimiter='\\t', encoding='utf-8')\n",
        "train_list = list(train_set['text'])\n",
        "train = train_set[train_set['task_1'] == 'HOF']\n",
        "train_frame = {0:train_list,1:list(train['text']),2:list(train['text'])}\n",
        "dict_train = {0:train_set['task_1'],1:train['task_2'],2:train['task_3']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTB0_2hBr_CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Test data prep\n",
        "test_set = pd.read_csv('test.tsv', delimiter='\\t', encoding='utf-8')\n",
        "test_list = list(test_set['text'])\n",
        "test = test_set[test_set['task_1'] == 'HOF']\n",
        "test_frame = {0:test_list,1:list(test['text']),2:list(test['text'])}\n",
        "dict_test = {0:test_set['task_1'],1:test['task_2'],2:test['task_3']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJEsHNeyGnVO",
        "colab_type": "code",
        "outputId": "3c95522c-b696-45d3-8c99-c36be3185047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        }
      },
      "source": [
        "!sudo apt install openjdk-8-jdk\n",
        "!sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!pip install language-check\n",
        "!pip install pycontractions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 64%\r\rReading package lists... 64%\r\rReading package lists... 64%\r\rReading package lists... 64%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 82%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jre x11-utils\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk openjdk-8-jre x11-utils\n",
            "0 upgraded, 8 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 1,676 kB/4,942 kB of archives.\n",
            "After this operation, 13.3 MB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u242-b08-0ubuntu3~18.04\n",
            "Ign:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u242-b08-0ubuntu3~18.04\n",
            "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u242-b08-0ubuntu3~18.04\n",
            "  404  Not Found [IP: 91.189.88.152 80]\n",
            "Err:2 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u242-b08-0ubuntu3~18.04\n",
            "  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/o/openjdk-8/openjdk-8-jre_8u242-b08-0ubuntu3~18.04_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/o/openjdk-8/openjdk-8-jdk_8u242-b08-0ubuntu3~18.04_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Requirement already satisfied: language-check in /usr/local/lib/python3.6/dist-packages (1.1)\n",
            "Requirement already satisfied: pycontractions in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pyemd>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (0.5.1)\n",
            "Requirement already satisfied: gensim>=2.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (3.6.0)\n",
            "Requirement already satisfied: language-check>=1.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (1.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pyemd>=0.4.4->pycontractions) (1.18.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.11.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.12.47)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.49.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.15.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3vSCJLUBe6U",
        "colab_type": "code",
        "outputId": "580cf3e3-8f29-4e62-a33d-db9704d97bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "## Pre-trained Glove Embeddings\n",
        "glove_model = api.load(\"glove-twitter-25\")\n",
        "contractions = Contractions(kv_model=glove_model)\n",
        "contractions.load_models()\n",
        "stop_words = stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: Word2number in /usr/local/lib/python3.6/dist-packages (1.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C7v9dfjE4yZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Preprocessing\n",
        "def pre(text):\n",
        "    \n",
        "    part = BeautifulSoup(text, \"html.parser\")\n",
        "    text = part.get_text(separator=\" \")\n",
        "    text = text.strip()\n",
        "    text = ' '.join(text.split())\n",
        "    text = unidecode.unidecode(text)\n",
        "    text = list(contractions.expand_texts([text], precise=True))[0]\n",
        "    text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text)\n",
        "    text = re.sub(r'\\d', '',text)\n",
        "    text = text.lower()\n",
        "\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tokens = tokenizer.tokenize(text) #tokenise text\n",
        "    \n",
        "    for w in tokens:\n",
        "      if w in stop_words:\n",
        "        tokens.remove(w)\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(i) for i in tokens]\n",
        "    text = \" \".join(tokens)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cu957E9ItU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = []\n",
        "for data in train_list:\n",
        "    for x in data.split(\" \"):\n",
        "        vocab.append(x)\n",
        "vocab = list(set(vocab))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(len(train_frame[i])):\n",
        "        train_frame[i][j] = pre(train_frame[i][j])\n",
        "    for j in range(len(test_frame[i])):\n",
        "        test_frame[i][j] = pre(test_frame[i][j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcf-7N_YKvwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect = CountVectorizer(analyzer = 'word',vocabulary = vocab)\n",
        "tfidf_transformer = TfidfTransformer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAkt3kynVme9",
        "colab_type": "code",
        "outputId": "df2a4044-1288-4583-abc9-a0272fa9c2ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "## Naive Bayes Classifier\n",
        "## We can use SGD/SVM or any other classifier here too\n",
        "dict_train_2 = {}\n",
        "dict_test_2 = {}\n",
        "for i in range(3):\n",
        "    count_train = count_vect.fit_transform(train_frame[i])\n",
        "    tfidf_train = tfidf_transformer.fit_transform(count_train)\n",
        "    count_test = count_vect.fit_transform(test_frame[i])\n",
        "    tfidf_test = tfidf_transformer.fit_transform(count_test)\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_2[i] = le.transform(dict_train[i].astype(str))\n",
        "    cl_2 = MultinomialNB().fit(tfidf_train, dict_train_2[i])\n",
        "    predictor = cl_2.predict(tfidf_test)\n",
        "    dict_test_2[i] = le.transform(dict_test[i].astype(str))\n",
        "    print(classification_report(dict_test_2[i], predictor))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.09      0.17       288\n",
            "           1       0.77      0.99      0.87       865\n",
            "\n",
            "    accuracy                           0.77      1153\n",
            "   macro avg       0.81      0.54      0.52      1153\n",
            "weighted avg       0.79      0.77      0.69      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.99      0.61       124\n",
            "           1       0.00      0.00      0.00        71\n",
            "           2       0.67      0.04      0.08        93\n",
            "\n",
            "    accuracy                           0.44       288\n",
            "   macro avg       0.37      0.34      0.23       288\n",
            "weighted avg       0.40      0.44      0.29       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92       245\n",
            "           1       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.85       288\n",
            "   macro avg       0.43      0.50      0.46       288\n",
            "weighted avg       0.72      0.85      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbgwWp1_Vobs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## With Pretrained embeddings - GloVe\n",
        "embed = {}\n",
        "with open(\"pre-trained.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        v = np.asarray(values[1:], \"float32\")\n",
        "        embed[word] = v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPrPZRUoXAGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_frame_2 = train_frame\n",
        "test_frame_2 = test_frame\n",
        "for j in range(3):\n",
        "    for i in range(len(train_frame_2[j])):\n",
        "        train_frame_2[j][i] = pre(train_frame_2[j][i])\n",
        "        zero = np.zeros((1,100))\n",
        "        for word in train_frame_2[j][i]:\n",
        "            try:\n",
        "                zero += embed[word]\n",
        "            except:\n",
        "                pass\n",
        "        train_frame_2[j][i] = zero/len(train_frame_2[j][i])\n",
        "    for i in range(len(test_frame_2[j])):\n",
        "        test_frame_2[j][i] = pre(test_frame_2[j][i])\n",
        "        zero = np.zeros((1,100))\n",
        "        for word in test_frame_2[j][i]:\n",
        "            try:\n",
        "                zero += embed[word]\n",
        "            except:\n",
        "                pass\n",
        "        test_frame_2[j][i] = zero/len(test_frame_2[j][i])\n",
        "for j in range(3):\n",
        "  train_frame_2[j] = np.array(train_frame_2[j]).reshape(-1,100)\n",
        "  test_frame_2[j] = np.array(test_frame_2[j]).reshape(-1,100) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXygtyVYYRFv",
        "colab_type": "code",
        "outputId": "bb7f4862-4fcf-4257-925d-3a7c0ffbc09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "source": [
        "dict_train_2=dict_train\n",
        "dict_test_2=dict_test\n",
        "for i in range(3):\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_2[i] = le.transform(dict_train_2[i].astype(str))\n",
        "    cl_3 = SGDClassifier().fit(train_frame_2[i], dict_train_2[i])\n",
        "    predictor = cl_3.predict(test_frame_2[i])\n",
        "    dict_test_2[i] = le.transform(dict_test_2[i].astype(str))\n",
        "    print(classification_report(dict_test_2[i], predictor))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      1.00      0.41       288\n",
            "           1       1.00      0.04      0.07       865\n",
            "\n",
            "    accuracy                           0.28      1153\n",
            "   macro avg       0.63      0.52      0.24      1153\n",
            "weighted avg       0.81      0.28      0.16      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.84      0.63       124\n",
            "           1       0.47      0.10      0.16        71\n",
            "           2       0.61      0.44      0.51        93\n",
            "\n",
            "    accuracy                           0.53       288\n",
            "   macro avg       0.53      0.46      0.44       288\n",
            "weighted avg       0.53      0.53      0.48       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       245\n",
            "           1       0.24      0.12      0.16        43\n",
            "\n",
            "    accuracy                           0.81       288\n",
            "   macro avg       0.55      0.53      0.53       288\n",
            "weighted avg       0.77      0.81      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}