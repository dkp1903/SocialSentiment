{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HateContentDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "otCbxMpJu0FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## IE-406 ML Project\n",
        "## Dushyant Pathak\n",
        "## Avi Gupta\n",
        "## Maharshi Patel\n",
        "## Pawan Baldaniya"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAY882jkR-ap",
        "colab_type": "code",
        "outputId": "6a83bf36-c203-4ef3-b738-4a1f6c2b00c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## Using Google Colab for GPU usage\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJEsHNeyGnVO",
        "colab_type": "code",
        "outputId": "596497f8-54f4-42c7-d41a-3ea51eca3a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "!sudo apt install openjdk-8-jdk\n",
        "!sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!pip install language-check\n",
        "!pip install pycontractions"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 63%\r\rReading package lists... 63%\r\rReading package lists... 63%\r\rReading package lists... 63%\r\rReading package lists... 71%\r\rReading package lists... 71%\r\rReading package lists... 71%\r\rReading package lists... 71%\r\rReading package lists... 77%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jdk is already the newest version (8u252-b09-1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "The folder you are executing pip from can no longer be found.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "The folder you are executing pip from can no longer be found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8crY2XLmvQXg",
        "colab_type": "code",
        "outputId": "463ca0ab-6a86-40c8-a116-0a0b2156ba31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "## Importing the necessary libraries\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "!pip install Unidecode\n",
        "!pip install Word2number\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import unidecode\n",
        "from word2number import w2n\n",
        "from pycontractions import Contractions\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "The folder you are executing pip from can no longer be found.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "The folder you are executing pip from can no longer be found.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn07RmHbBCIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/hate-con-proj')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIAviZW8rnn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train data prep\n",
        "train_set = pd.read_csv('train.tsv', delimiter='\\t', encoding='utf-8')\n",
        "train_list = list(train_set['text'])\n",
        "train = train_set[train_set['task_1'] == 'HOF']\n",
        "train_frame = {0:train_list,1:list(train['text']),2:list(train['text'])}\n",
        "dict_train = {0:train_set['task_1'],1:train['task_2'],2:train['task_3']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTB0_2hBr_CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Test data prep\n",
        "test_set = pd.read_csv('test.tsv', delimiter='\\t', encoding='utf-8')\n",
        "test_list = list(test_set['text'])\n",
        "test = test_set[test_set['task_1'] == 'HOF']\n",
        "test_frame = {0:test_list,1:list(test['text']),2:list(test['text'])}\n",
        "dict_test = {0:test_set['task_1'],1:test['task_2'],2:test['task_3']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS43581Te_PL",
        "colab_type": "text"
      },
      "source": [
        "We have to remove contractions('ve->have) and stopwords(like the,at, in)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3vSCJLUBe6U",
        "colab_type": "code",
        "outputId": "1b34b552-5ae5-4cdc-e7a4-bf2c972f7e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "## Pre-trained Glove Embeddings\n",
        "glove_model = api.load(\"glove-twitter-25\")\n",
        "contractions = Contractions(kv_model=glove_model)\n",
        "contractions.load_models()\n",
        "stop_words = stopwords.words('english')\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C7v9dfjE4yZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Preprocessing\n",
        "def pre(text):\n",
        "    \n",
        "    ## parsing the text\n",
        "    part = BeautifulSoup(text, \"html.parser\")\n",
        "    text = part.get_text(separator=\" \")\n",
        "    text = text.strip()\n",
        "    text = ' '.join(text.split())\n",
        "    text = unidecode.unidecode(text)\n",
        "    ## expanding the contractions\n",
        "    text = list(contractions.expand_texts([text], precise=True))[0]\n",
        "    text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text)\n",
        "    text = re.sub(r'\\d', '',text)\n",
        "    text = text.lower()\n",
        "\n",
        "    ## tokenization using TweetTokenizer\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tokens = tokenizer.tokenize(text) #tokenise text\n",
        "    \n",
        "    for w in tokens:\n",
        "      if w in stop_words:\n",
        "        tokens.remove(w)\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(i) for i in tokens]\n",
        "    text = \" \".join(tokens)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cu957E9ItU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = []\n",
        "for data in train_list:\n",
        "    for x in data.split(\" \"):\n",
        "        vocab.append(x)\n",
        "vocab = list(set(vocab))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(len(train_frame[i])):\n",
        "        train_frame[i][j] = pre(train_frame[i][j])\n",
        "    for j in range(len(test_frame[i])):\n",
        "        test_frame[i][j] = pre(test_frame[i][j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcf-7N_YKvwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tf-IDF\n",
        "count_vect = CountVectorizer(analyzer = 'word',vocabulary = vocab)\n",
        "tfidf_transformer = TfidfTransformer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAkt3kynVme9",
        "colab_type": "code",
        "outputId": "f2ca1369-e9c6-4e4b-998b-ad1ed3ecd19f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "## Naive Bayes Classifier\n",
        "\n",
        "dict_train_2 = {}\n",
        "dict_test_2 = {}\n",
        "for i in range(3):\n",
        "    count_train = count_vect.fit_transform(train_frame[i])\n",
        "    tfidf_train = tfidf_transformer.fit_transform(count_train)\n",
        "    count_test = count_vect.fit_transform(test_frame[i])\n",
        "    tfidf_test = tfidf_transformer.fit_transform(count_test)\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_2[i] = le.transform(dict_train[i].astype(str))\n",
        "    cl_2 = MultinomialNB().fit(tfidf_train, dict_train_2[i])\n",
        "    predictor = cl_2.predict(tfidf_test)\n",
        "    dict_test_2[i] = le.transform(dict_test[i].astype(str))\n",
        "    print(classification_report(dict_test_2[i], predictor))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.09      0.17       288\n",
            "           1       0.77      0.99      0.87       865\n",
            "\n",
            "    accuracy                           0.77      1153\n",
            "   macro avg       0.81      0.54      0.52      1153\n",
            "weighted avg       0.79      0.77      0.69      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.99      0.61       124\n",
            "           1       0.00      0.00      0.00        71\n",
            "           2       0.67      0.04      0.08        93\n",
            "\n",
            "    accuracy                           0.44       288\n",
            "   macro avg       0.37      0.34      0.23       288\n",
            "weighted avg       0.40      0.44      0.29       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92       245\n",
            "           1       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.85       288\n",
            "   macro avg       0.43      0.50      0.46       288\n",
            "weighted avg       0.72      0.85      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDJdufeQfb4b",
        "colab_type": "code",
        "outputId": "31b9dad4-a8b3-40bf-8b16-31011dd82010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "source": [
        "## SGD(Stochastic Gradient Descent) Classifier\n",
        "dict_train_2 = {}\n",
        "dict_test_2 = {}\n",
        "for i in range(3):\n",
        "    count_train = count_vect.fit_transform(train_frame[i])\n",
        "    tfidf_train = tfidf_transformer.fit_transform(count_train)\n",
        "    count_test = count_vect.fit_transform(test_frame[i])\n",
        "    tfidf_test = tfidf_transformer.fit_transform(count_test)\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_2[i] = le.transform(dict_train[i].astype(str))\n",
        "    cl_2 = SGDClassifier().fit(tfidf_train, dict_train_2[i])\n",
        "    predictor = cl_2.predict(tfidf_test)\n",
        "    dict_test_2[i] = le.transform(dict_test[i].astype(str))\n",
        "    print(classification_report(dict_test_2[i], predictor))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.56      0.49       288\n",
            "           1       0.84      0.76      0.79       865\n",
            "\n",
            "    accuracy                           0.71      1153\n",
            "   macro avg       0.63      0.66      0.64      1153\n",
            "weighted avg       0.74      0.71      0.72      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.60      0.55       124\n",
            "           1       0.36      0.23      0.28        71\n",
            "           2       0.55      0.59      0.57        93\n",
            "\n",
            "    accuracy                           0.50       288\n",
            "   macro avg       0.47      0.47      0.47       288\n",
            "weighted avg       0.49      0.50      0.49       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.95      0.90       245\n",
            "           1       0.20      0.07      0.10        43\n",
            "\n",
            "    accuracy                           0.82       288\n",
            "   macro avg       0.53      0.51      0.50       288\n",
            "weighted avg       0.76      0.82      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbgwWp1_Vobs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## With Pretrained embeddings - GloVe\n",
        "embed = {}\n",
        "with open(\"pre-trained.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        v = np.asarray(values[1:], \"float32\")\n",
        "        embed[word] = v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YUQqR_cJoI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train data prep\n",
        "train_set = pd.read_csv('train.tsv', delimiter='\\t', encoding='utf-8')\n",
        "train_list = list(train_set['text'])\n",
        "train = train_set[train_set['task_1'] == 'HOF']\n",
        "train_frame = {0:train_list,1:list(train['text']),2:list(train['text'])}\n",
        "dict_train = {0:train_set['task_1'],1:train['task_2'],2:train['task_3']}\n",
        "## Test data prep\n",
        "test_set = pd.read_csv('test.tsv', delimiter='\\t', encoding='utf-8')\n",
        "test_list = list(test_set['text'])\n",
        "test = test_set[test_set['task_1'] == 'HOF']\n",
        "test_frame = {0:test_list,1:list(test['text']),2:list(test['text'])}\n",
        "dict_test = {0:test_set['task_1'],1:test['task_2'],2:test['task_3']}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQUtPSN5Kja-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = []\n",
        "for data in train_list:\n",
        "    for x in data.split(\" \"):\n",
        "        vocab.append(x)\n",
        "vocab = list(set(vocab))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(len(train_frame[i])):\n",
        "        train_frame[i][j] = pre(train_frame[i][j])\n",
        "    for j in range(len(test_frame[i])):\n",
        "        test_frame[i][j] = pre(test_frame[i][j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPrPZRUoXAGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_frame_2 = train_frame\n",
        "test_frame_2 = test_frame\n",
        "for j in range(3):\n",
        "    for i in range(len(train_frame_2[j])):\n",
        "        train_frame_2[j][i] = pre(train_frame_2[j][i])\n",
        "        zero = np.zeros((1,100))\n",
        "        for word in train_frame_2[j][i]:\n",
        "            try:\n",
        "                zero += embed[word]\n",
        "            except:\n",
        "                pass\n",
        "        train_frame_2[j][i] = zero/len(train_frame_2[j][i])\n",
        "    for i in range(len(test_frame_2[j])):\n",
        "        test_frame_2[j][i] = pre(test_frame_2[j][i])\n",
        "        zero = np.zeros((1,100))\n",
        "        for word in test_frame_2[j][i]:\n",
        "            try:\n",
        "                zero += embed[word]\n",
        "            except:\n",
        "                pass\n",
        "        test_frame_2[j][i] = zero/len(test_frame_2[j][i])\n",
        "for j in range(3):\n",
        "  train_frame_2[j] = np.array(train_frame_2[j]).reshape(-1,100)\n",
        "  test_frame_2[j] = np.array(test_frame_2[j]).reshape(-1,100) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXygtyVYYRFv",
        "colab_type": "code",
        "outputId": "db1be277-fc06-42f6-a983-96024e18ad05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "source": [
        "dict_train_2=dict_train\n",
        "dict_test_2=dict_test\n",
        "for i in range(3):\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_2[i] = le.transform(dict_train_2[i].astype(str))\n",
        "    cl_3 = SGDClassifier().fit(train_frame_2[i], dict_train_2[i])\n",
        "    predictor = cl_3.predict(test_frame_2[i])\n",
        "    dict_test_2[i] = le.transform(dict_test_2[i].astype(str))\n",
        "    print(classification_report(dict_test_2[i], predictor))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.39      0.41       288\n",
            "           1       0.80      0.83      0.82       865\n",
            "\n",
            "    accuracy                           0.72      1153\n",
            "   macro avg       0.62      0.61      0.61      1153\n",
            "weighted avg       0.71      0.72      0.71      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.93      0.64       124\n",
            "           1       0.60      0.04      0.08        71\n",
            "           2       0.65      0.34      0.45        93\n",
            "\n",
            "    accuracy                           0.52       288\n",
            "   macro avg       0.58      0.44      0.39       288\n",
            "weighted avg       0.57      0.52      0.44       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92       245\n",
            "           1       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.85       288\n",
            "   macro avg       0.43      0.50      0.46       288\n",
            "weighted avg       0.72      0.85      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKUdtUhFppsb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "35bf7f10-14d7-43ed-9730-a2400b1ad1fc"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-04 16:35:35--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.65.54\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.65.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  90.4MB/s    in 19s     \n",
            "\n",
            "2020-06-04 16:35:55 (81.0 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUx2u6jwpwKc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "0b342559-0c5f-4503-cdf9-73410dc5adb3"
      },
      "source": [
        "!pip install gensim\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.4)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.0.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.13.19)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.19 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.16.19)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpRrSH21KIer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train data prep\n",
        "train_set = pd.read_csv('train.tsv', delimiter='\\t', encoding='utf-8')\n",
        "train_list = list(train_set['text'])\n",
        "train = train_set[train_set['task_1'] == 'HOF']\n",
        "train_frame = {0:train_list,1:list(train['text']),2:list(train['text'])}\n",
        "dict_train = {0:train_set['task_1'],1:train['task_2'],2:train['task_3']}\n",
        "## Test data prep\n",
        "test_set = pd.read_csv('test.tsv', delimiter='\\t', encoding='utf-8')\n",
        "test_list = list(test_set['text'])\n",
        "test = test_set[test_set['task_1'] == 'HOF']\n",
        "test_frame = {0:test_list,1:list(test['text']),2:list(test['text'])}\n",
        "dict_test = {0:test_set['task_1'],1:test['task_2'],2:test['task_3']}\n",
        "\n",
        "vocab = []\n",
        "for data in train_list:\n",
        "    for x in data.split(\" \"):\n",
        "        vocab.append(x)\n",
        "vocab = list(set(vocab))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(len(train_frame[i])):\n",
        "        train_frame[i][j] = pre(train_frame[i][j])\n",
        "    for j in range(len(test_frame[i])):\n",
        "        test_frame[i][j] = pre(test_frame[i][j])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWGHYJc6q-Ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## With Pretrained embeddings - Word2Vec\n",
        "embed = {}\n",
        "with open(\"model.txt\", 'r', encoding = 'utf-8', errors='ignore') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        v = np.asarray(values[1:], \"float32\")\n",
        "        embed[word] = v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXywmLnKKwC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_frame_3 = train_frame\n",
        "test_frame_3 = test_frame\n",
        "for j in range(3):\n",
        "    for i in range(len(train_frame_3[j])):\n",
        "        train_frame_3[j][i] = pre(train_frame_3[j][i])\n",
        "        zero = np.zeros((1,100))\n",
        "        for word in train_frame_3[j][i]:\n",
        "            try:\n",
        "                zero += embed[word]\n",
        "            except:\n",
        "                pass\n",
        "        train_frame_3[j][i] = zero/len(train_frame_3[j][i])\n",
        "    for i in range(len(test_frame_3[j])):\n",
        "        test_frame_3[j][i] = pre(test_frame_3[j][i])\n",
        "        zero = np.zeros((1,100))\n",
        "        for word in test_frame_3[j][i]:\n",
        "            try:\n",
        "                zero += embed[word]\n",
        "            except:\n",
        "                pass\n",
        "        test_frame_3[j][i] = zero/len(test_frame_3[j][i])\n",
        "for j in range(3):\n",
        "  train_frame_3[j] = np.array(train_frame_3[j]).reshape(-1,100)\n",
        "  test_frame_3[j] = np.array(test_frame_3[j]).reshape(-1,100) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1wFucgVKxIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "6c402f6a-e31f-4476-8dfd-a4b1286e6c7c"
      },
      "source": [
        "dict_train_3=dict_train\n",
        "dict_test_3=dict_test\n",
        "for i in range(3):\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_3[i] = le.transform(dict_train_3[i].astype(str))\n",
        "    cl_4 = SGDClassifier().fit(train_frame_3[i], dict_train_3[i])\n",
        "    predictor = cl_4.predict(test_frame_3[i])\n",
        "    dict_test_3[i] = le.transform(dict_test_3[i].astype(str))\n",
        "    print(classification_report(dict_test_3[i], predictor))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.01      0.03       288\n",
            "           1       0.75      0.99      0.86       865\n",
            "\n",
            "    accuracy                           0.75      1153\n",
            "   macro avg       0.60      0.50      0.44      1153\n",
            "weighted avg       0.67      0.75      0.65      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.62      0.57       124\n",
            "           1       0.00      0.00      0.00        71\n",
            "           2       0.45      0.68      0.54        93\n",
            "\n",
            "    accuracy                           0.49       288\n",
            "   macro avg       0.32      0.43      0.37       288\n",
            "weighted avg       0.37      0.49      0.42       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92       245\n",
            "           1       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.85       288\n",
            "   macro avg       0.43      0.50      0.46       288\n",
            "weighted avg       0.72      0.85      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}