{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HateContentDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "otCbxMpJu0FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## IE-406 ML Project\n",
        "## Dushyant Pathak\n",
        "## Avi Gupta\n",
        "## Maharshi Patel\n",
        "## Pawan Baldaniya"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAY882jkR-ap",
        "colab_type": "code",
        "outputId": "4ae524cd-2f4c-43e6-d770-abab1f3c61a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "## Using Google Colab for GPU usage\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJEsHNeyGnVO",
        "colab_type": "code",
        "outputId": "3e0d9a0d-a66d-4048-b29f-fdd9951bf1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sudo apt install openjdk-8-jdk\n",
        "!sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!pip install language-check\n",
        "!pip install pycontractions"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless\n",
            "  openjdk-8-jre openjdk-8-jre-headless x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 40.7 MB of archives.\n",
            "After this operation, 153 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u252-b09-1~18.04 [27.5 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u252-b09-1~18.04 [69.8 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u252-b09-1~18.04 [8,250 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u252-b09-1~18.04 [1,622 kB]\n",
            "Fetched 40.7 MB in 1s (68.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 144439 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../1-fonts-dejavu-core_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../2-fonts-dejavu-extra_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../3-x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../4-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../5-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../6-openjdk-8-jre-headless_8u252-b09-1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u252-b09-1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../7-openjdk-8-jre_8u252-b09-1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u252-b09-1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../8-openjdk-8-jdk-headless_8u252-b09-1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u252-b09-1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-8-jdk_8u252-b09-1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u252-b09-1~18.04) ...\n",
            "Setting up fonts-dejavu-core (2.37-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u252-b09-1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u252-b09-1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Setting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u252-b09-1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u252-b09-1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "Collecting language-check\n",
            "  Using cached https://files.pythonhosted.org/packages/97/45/0fd1d3683d6129f30fa09143fa383cdf6dff8bc0d1648f2cf156109cb772/language-check-1.1.tar.gz\n",
            "Building wheels for collected packages: language-check\n",
            "  Building wheel for language-check (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for language-check: filename=language_check-1.1-cp36-none-any.whl size=90190899 sha256=6c21addb9dce2ecf7c4a7a6c75bf3be256fc3b61011390389ab895c327f66eec\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/46/82/90a89c23eac1837364ed7217a9eed71bc9e6ad4825be93968e\n",
            "Successfully built language-check\n",
            "Installing collected packages: language-check\n",
            "Successfully installed language-check-1.1\n",
            "Collecting pycontractions\n",
            "  Using cached https://files.pythonhosted.org/packages/a6/f5/d3ec9491c530cbc03af32ca2c6b69b0e89660daeb2856b485d90f9d82e5e/pycontractions-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: language-check>=1.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (1.1)\n",
            "Requirement already satisfied: pyemd>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (0.5.1)\n",
            "Requirement already satisfied: gensim>=2.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (3.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pyemd>=0.4.4->pycontractions) (1.18.4)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.4.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.13.13)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.23.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.16.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.9)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.8.1)\n",
            "Installing collected packages: pycontractions\n",
            "Successfully installed pycontractions-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8crY2XLmvQXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "62474cb1-1f74-4288-fa41-9936327fa3b4"
      },
      "source": [
        "## Importing the necessary libraries\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "!pip install Unidecode\n",
        "!pip install Word2number\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import unidecode\n",
        "from word2number import w2n\n",
        "from pycontractions import Contractions\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycontractions in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pyemd>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (0.5.1)\n",
            "Requirement already satisfied: gensim>=2.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (3.6.0)\n",
            "Requirement already satisfied: language-check>=1.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (1.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pyemd>=0.4.4->pycontractions) (1.18.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (2.0.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.13.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.23.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.49.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.16.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.8.1)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: Word2number in /usr/local/lib/python3.6/dist-packages (1.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn07RmHbBCIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/hate-con-proj')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIAviZW8rnn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train data prep\n",
        "train_set = pd.read_csv('train.tsv', delimiter='\\t', encoding='utf-8')\n",
        "train_list = list(train_set['text'])\n",
        "train = train_set[train_set['task_1'] == 'HOF']\n",
        "train_frame = {0:train_list,1:list(train['text']),2:list(train['text'])}\n",
        "dict_train = {0:train_set['task_1'],1:train['task_2'],2:train['task_3']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTB0_2hBr_CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Test data prep\n",
        "test_set = pd.read_csv('test.tsv', delimiter='\\t', encoding='utf-8')\n",
        "test_list = list(test_set['text'])\n",
        "test = test_set[test_set['task_1'] == 'HOF']\n",
        "test_frame = {0:test_list,1:list(test['text']),2:list(test['text'])}\n",
        "dict_test = {0:test_set['task_1'],1:test['task_2'],2:test['task_3']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS43581Te_PL",
        "colab_type": "text"
      },
      "source": [
        "We have to remove contractions('ve->have) and stopwords(like the,a t, in)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3vSCJLUBe6U",
        "colab_type": "code",
        "outputId": "efde4716-13e6-45b0-f294-58f58c288785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "## Pre-trained Glove Embeddings\n",
        "glove_model = api.load(\"glove-twitter-25\")\n",
        "contractions = Contractions(kv_model=glove_model)\n",
        "contractions.load_models()\n",
        "stop_words = stopwords.words('english')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================------------------] 65.4% 68.6/104.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C7v9dfjE4yZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Preprocessing\n",
        "def pre(text):\n",
        "    \n",
        "    ## parsing the text\n",
        "    part = BeautifulSoup(text, \"html.parser\")\n",
        "    text = part.get_text(separator=\" \")\n",
        "    text = text.strip()\n",
        "    text = ' '.join(text.split())\n",
        "    text = unidecode.unidecode(text)\n",
        "    ## expanding the contractions\n",
        "    text = list(contractions.expand_texts([text], precise=True))[0]\n",
        "    text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text)\n",
        "    text = re.sub(r'\\d', '',text)\n",
        "    text = text.lower()\n",
        "\n",
        "    ## tokenization using TweetTokenizer\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tokens = tokenizer.tokenize(text) #tokenise text\n",
        "    \n",
        "    for w in tokens:\n",
        "      if w in stop_words:\n",
        "        tokens.remove(w)\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(i) for i in tokens]\n",
        "    text = \" \".join(tokens)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cu957E9ItU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = []\n",
        "for data in train_list:\n",
        "    for x in data.split(\" \"):\n",
        "        vocab.append(x)\n",
        "vocab = list(set(vocab))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(len(train_frame[i])):\n",
        "        train_frame[i][j] = pre(train_frame[i][j])\n",
        "    for j in range(len(test_frame[i])):\n",
        "        test_frame[i][j] = pre(test_frame[i][j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcf-7N_YKvwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tf-IDF\n",
        "count_vect = CountVectorizer(analyzer = 'word',vocabulary = vocab)\n",
        "tfidf_transformer = TfidfTransformer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAkt3kynVme9",
        "colab_type": "code",
        "outputId": "a27245ec-121b-4be6-d1a1-6a920b346916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "## Naive Bayes Classifier\n",
        "\n",
        "dict_train_2 = {}\n",
        "dict_test_2 = {}\n",
        "for i in range(3):\n",
        "    count_train = count_vect.fit_transform(train_frame[i])\n",
        "    tfidf_train = tfidf_transformer.fit_transform(count_train)\n",
        "    count_test = count_vect.fit_transform(test_frame[i])\n",
        "    tfidf_test = tfidf_transformer.fit_transform(count_test)\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_2[i] = le.transform(dict_train[i].astype(str))\n",
        "    cl_2 = MultinomialNB().fit(tfidf_train, dict_train_2[i])\n",
        "    predictor = cl_2.predict(tfidf_test)\n",
        "    dict_test_2[i] = le.transform(dict_test[i].astype(str))\n",
        "    print(classification_report(dict_test_2[i], predictor))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.09      0.17       288\n",
            "           1       0.77      0.99      0.87       865\n",
            "\n",
            "    accuracy                           0.77      1153\n",
            "   macro avg       0.81      0.54      0.52      1153\n",
            "weighted avg       0.79      0.77      0.69      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.99      0.61       124\n",
            "           1       0.00      0.00      0.00        71\n",
            "           2       0.67      0.04      0.08        93\n",
            "\n",
            "    accuracy                           0.44       288\n",
            "   macro avg       0.37      0.34      0.23       288\n",
            "weighted avg       0.40      0.44      0.29       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92       245\n",
            "           1       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.85       288\n",
            "   macro avg       0.43      0.50      0.46       288\n",
            "weighted avg       0.72      0.85      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDJdufeQfb4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "cc6221d3-713a-4bf5-bfee-04b9ce4d84a8"
      },
      "source": [
        "## SGD(Stochastic Gradient Descent) Classifier\n",
        "dict_train_2 = {}\n",
        "dict_test_2 = {}\n",
        "for i in range(3):\n",
        "    count_train = count_vect.fit_transform(train_frame[i])\n",
        "    tfidf_train = tfidf_transformer.fit_transform(count_train)\n",
        "    count_test = count_vect.fit_transform(test_frame[i])\n",
        "    tfidf_test = tfidf_transformer.fit_transform(count_test)\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_2[i] = le.transform(dict_train[i].astype(str))\n",
        "    cl_2 = SGDClassifier().fit(tfidf_train, dict_train_2[i])\n",
        "    predictor = cl_2.predict(tfidf_test)\n",
        "    dict_test_2[i] = le.transform(dict_test[i].astype(str))\n",
        "    print(classification_report(dict_test_2[i], predictor))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.55      0.49       288\n",
            "           1       0.84      0.76      0.80       865\n",
            "\n",
            "    accuracy                           0.71      1153\n",
            "   macro avg       0.63      0.66      0.64      1153\n",
            "weighted avg       0.74      0.71      0.72      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.60      0.55       124\n",
            "           1       0.35      0.23      0.27        71\n",
            "           2       0.55      0.58      0.57        93\n",
            "\n",
            "    accuracy                           0.50       288\n",
            "   macro avg       0.47      0.47      0.46       288\n",
            "weighted avg       0.48      0.50      0.49       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.95      0.90       245\n",
            "           1       0.14      0.05      0.07        43\n",
            "\n",
            "    accuracy                           0.82       288\n",
            "   macro avg       0.50      0.50      0.48       288\n",
            "weighted avg       0.74      0.82      0.77       288\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbgwWp1_Vobs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## With Pretrained embeddings - GloVe\n",
        "embed = {}\n",
        "with open(\"pre-trained.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        v = np.asarray(values[1:], \"float32\")\n",
        "        embed[word] = v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPrPZRUoXAGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_frame_2 = train_frame\n",
        "test_frame_2 = test_frame\n",
        "for j in range(3):\n",
        "    for i in range(len(train_frame_2[j])):\n",
        "        train_frame_2[j][i] = pre(train_frame_2[j][i])\n",
        "        zero = np.zeros((1,100))\n",
        "        for word in train_frame_2[j][i]:\n",
        "            try:\n",
        "                zero += embed[word]\n",
        "            except:\n",
        "                pass\n",
        "        train_frame_2[j][i] = zero/len(train_frame_2[j][i])\n",
        "    for i in range(len(test_frame_2[j])):\n",
        "        test_frame_2[j][i] = pre(test_frame_2[j][i])\n",
        "        zero = np.zeros((1,100))\n",
        "        for word in test_frame_2[j][i]:\n",
        "            try:\n",
        "                zero += embed[word]\n",
        "            except:\n",
        "                pass\n",
        "        test_frame_2[j][i] = zero/len(test_frame_2[j][i])\n",
        "for j in range(3):\n",
        "  train_frame_2[j] = np.array(train_frame_2[j]).reshape(-1,100)\n",
        "  test_frame_2[j] = np.array(test_frame_2[j]).reshape(-1,100) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXygtyVYYRFv",
        "colab_type": "code",
        "outputId": "18ac0a8d-d94b-4f54-e0e4-8d833c60a75d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "dict_train_2=dict_train\n",
        "dict_test_2=dict_test\n",
        "for i in range(3):\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(dict_train[i]))\n",
        "    dict_train_2[i] = le.transform(dict_train_2[i].astype(str))\n",
        "    cl_3 = SGDClassifier().fit(train_frame_2[i], dict_train_2[i])\n",
        "    predictor = cl_3.predict(test_frame_2[i])\n",
        "    dict_test_2[i] = le.transform(dict_test_2[i].astype(str))\n",
        "    print(classification_report(dict_test_2[i], predictor))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.36      0.40       288\n",
            "           1       0.80      0.85      0.83       865\n",
            "\n",
            "    accuracy                           0.73      1153\n",
            "   macro avg       0.62      0.61      0.61      1153\n",
            "weighted avg       0.71      0.73      0.72      1153\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.91      0.64       124\n",
            "           1       0.00      0.00      0.00        71\n",
            "           2       0.61      0.40      0.48        93\n",
            "\n",
            "    accuracy                           0.52       288\n",
            "   macro avg       0.37      0.44      0.37       288\n",
            "weighted avg       0.41      0.52      0.43       288\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92       245\n",
            "           1       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.85       288\n",
            "   macro avg       0.43      0.50      0.46       288\n",
            "weighted avg       0.72      0.85      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}